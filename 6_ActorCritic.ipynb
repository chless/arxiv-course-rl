{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g3XLnTNg7gqD"
      },
      "source": [
        "### Utils"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w3U7znRFH4UP",
        "outputId": "8b163f67-6c98-4164-bc35-9ff2336b82b2"
      },
      "source": [
        "!apt-get install -y xvfb python-opengl x11-utils\n",
        "!pip install pyvirtualdisplay==0.2.5"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "x11-utils is already the newest version (7.7+3build1).\n",
            "python-opengl is already the newest version (3.1.0+dfsg-1).\n",
            "xvfb is already the newest version (2:1.19.6-1ubuntu4.9).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 37 not upgraded.\n",
            "Requirement already satisfied: pyvirtualdisplay==0.2.5 in /usr/local/lib/python3.7/dist-packages (0.2.5)\n",
            "Requirement already satisfied: EasyProcess in /usr/local/lib/python3.7/dist-packages (from pyvirtualdisplay==0.2.5) (0.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ArJh7UR4JDXF"
      },
      "source": [
        "import gym\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.distributions import Categorical, Normal\n",
        "\n",
        "import random, datetime, gym, os, time, psutil, cv2\n",
        "import numpy as np\n",
        "from torchsummary import summary as summary_\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import animation\n",
        "from IPython.display import display, HTML\n",
        "from collections import deque, namedtuple\n",
        "\n",
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lgcqIEwDw3c4"
      },
      "source": [
        "from pyvirtualdisplay import Display\n",
        "v_display = Display(visible=0, size=(400, 300))\n",
        "v_display.start()\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=np.VisibleDeprecationWarning)\n",
        "\n",
        "def display_animation(anim):\n",
        "    plt.close(anim._fig)\n",
        "    return HTML(anim.to_jshtml())\n",
        "\n",
        "def display_frames_as_gif(frames):\n",
        "    patch = plt.imshow(frames[0])\n",
        "    plt.axis('off')\n",
        "\n",
        "    def animate(i):\n",
        "        patch.set_data(frames[i])\n",
        "    \n",
        "    anim = animation.FuncAnimation(\n",
        "        plt.gcf(), animate, frames=len(frames), interval=10)\n",
        "    display(display_animation(anim))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F0Q7TeaaI86z"
      },
      "source": [
        "# Actor Critic Model\n",
        "- mountain car\n",
        "- discrete action space"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8SETe-njI_99"
      },
      "source": [
        "class ActorCritic(nn.Module):\n",
        "    def __init__(self, n_state, n_action):\n",
        "        super(ActorCritic, self).__init__()\n",
        "        self.shared_layer = nn.Linear(n_state,256)\n",
        "        self.policy_layer = nn.Linear(256,n_action)\n",
        "        self.value_layer = nn.Linear(256,1)\n",
        "        self.optimizer = optim.Adam(self.parameters(), lr=learning_rate)\n",
        "        self.data = []\n",
        "        \n",
        "    def policy(self, state, softmax_dim = 0):\n",
        "        x = F.relu(self.shared_layer(state))\n",
        "        x = self.policy_layer(x)\n",
        "        prob = F.softmax(x, dim=softmax_dim)\n",
        "        return Categorical(prob)\n",
        "    \n",
        "    def get_action(self, state):\n",
        "        dist = self.policy(torch.from_numpy(state).float())\n",
        "        action = dist.sample().item()\n",
        "        return action\n",
        "    \n",
        "    def value(self, state):\n",
        "        x = F.relu(self.shared_layer(state))\n",
        "        value = self.value_layer(x)\n",
        "        return value\n",
        "    \n",
        "    def save(self, transition):\n",
        "        self.data.append(transition)\n",
        "        \n",
        "    def get_batch(self):\n",
        "        state_list, action_list, reward_list, next_state_list, done_list = [], [], [], [], []\n",
        "        for experience in self.data:\n",
        "            state, action, reward, next_state, done = experience\n",
        "            state_list.append(state)\n",
        "            action_list.append([action])\n",
        "            reward_list.append([reward/100.0])\n",
        "            next_state_list.append(next_state)\n",
        "            done_list.append([0.0 if done else 1.0])\n",
        "        \n",
        "        state_batch = torch.tensor(state_list, dtype=torch.float)\n",
        "        action_batch = torch.tensor(action_list)\n",
        "        reward_batch = torch.tensor(reward_list, dtype=torch.float)\n",
        "        next_state_batch = torch.tensor(next_state_list, dtype=torch.float)\n",
        "        done_batch = torch.tensor(done_list, dtype=torch.float)\n",
        "        self.data = []\n",
        "\n",
        "        return state_batch, action_batch, reward_batch, next_state_batch, done_batch\n",
        "  \n",
        "    def update(self):\n",
        "        state, action, reward, next_state, done = self.get_batch()\n",
        "        td_target = reward + gamma * self.value(next_state) * done\n",
        "        td_error = td_target - self.value(state)\n",
        "        \n",
        "        dist = self.policy(state, softmax_dim=1)\n",
        "        prob_action = dist.probs.gather(1, action)\n",
        "        loss_actor = - torch.log(prob_action) * td_error.detach()\n",
        "        # loss_critic = F.smooth_l1_loss(self.value(state), td_target.detach())\n",
        "        loss_critic = (self.value(state) - td_target.detach())**2\n",
        "        loss = loss_actor + loss_critic\n",
        "\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.mean().backward()\n",
        "        self.optimizer.step()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zgsh3nRbJH3u"
      },
      "source": [
        "# Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mn65Ay83XSRH"
      },
      "source": [
        "#Hyperparameters\n",
        "gamma = 0.98\n",
        "update_every = 10\n",
        "learning_rate=0.0002"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SLOkEioOT0eF"
      },
      "source": [
        "def train(agent, env, eval_env, print_every=200, num_episodes=10000, learning_rate=0.0002, display=True):\n",
        "    episodic_reward = np.zeros([num_episodes])\n",
        "    for n_epi in range(num_episodes):\n",
        "        done = False\n",
        "        state = env.reset()\n",
        "        while not done:\n",
        "            for t in range(update_every):\n",
        "                action = agent.get_action(state)\n",
        "                next_state, reward, done, info = env.step(action)\n",
        "                agent.save((state, action, reward, next_state,done))\n",
        "                state = next_state\n",
        "                episodic_reward[n_epi] += reward\n",
        "                if done:\n",
        "                    break                     \n",
        "            agent.update()\n",
        "        if (n_epi==0) or ((n_epi+1) % print_every == 0):\n",
        "            print(\"# of episode :{}, return : {:.1f}\".format(n_epi, episodic_reward[n_epi]))\n",
        "            if display:\n",
        "                print(f\"[Eval. start] step:[{n_epi + 1}/{num_episodes}]\")\n",
        "\n",
        "                state, done, ep_ret, ep_len = eval_env.reset(), False, 0, 0\n",
        "                frames = []\n",
        "                while not done:\n",
        "                    action = agent.get_action(state)\n",
        "                    state, reward, done, _ = eval_env.step(action)\n",
        "                    frame = eval_env.render(mode='rgb_array')\n",
        "                    texted_frame = cv2.putText(\n",
        "                        img=np.copy(frame),\n",
        "                        text='epoch:{}'.format(n_epi+1, ep_len),\n",
        "                        org=(300,100), fontFace=3, fontScale=1, color=(0,0,255), thickness=3)\n",
        "                    frames.append(texted_frame)\n",
        "                    ep_ret += reward # return\n",
        "                    ep_len += 1 # length \n",
        "                print(f\"[Eval. done]\")\n",
        "                \n",
        "                #Display GIF\n",
        "                display_frames_as_gif(frames)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MW8Pv1l69YGS"
      },
      "source": [
        "cart_pole = gym.make('CartPole-v0')\n",
        "cart_pole_eval = gym.make('CartPole-v0')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HwuF3KgD9NM5",
        "outputId": "16938b93-5c32-4d55-e4f0-2bf40380cdd4"
      },
      "source": [
        "cart_pole.observation_space"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Box(-3.4028234663852886e+38, 3.4028234663852886e+38, (4,), float32)"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zy597RIe9Pfp",
        "outputId": "eb5b8da3-922e-4698-f28b-5c60327ced7d"
      },
      "source": [
        "cart_pole.action_space"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Discrete(2)"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M29KXKIbcqL-"
      },
      "source": [
        "actor_critic = ActorCritic(n_state=4, n_action=2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3nX51uWlcxsV",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "6928bfc5-8762-4fcc-fbe1-dc31ff0a84c0"
      },
      "source": [
        "train(actor_critic, cart_pole, cart_pole_eval, 200, 1000, 0.0002)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v0k9tdl6nzAk"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    }
  ]
}